{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from word2number import w2n\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, label_name):\n",
    "    with open(filename, 'r') as f:\n",
    "        return np.array([[eval(data_point)['question'], eval(data_point)[label_name]] for data_point in f.read().splitlines() if label_name in eval(data_point)])\n",
    "    \n",
    "def is_spelled_out_number(text):\n",
    "    try:\n",
    "        w2n.word_to_num(text)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_function(text): # change to regex\n",
    "    if '()' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_snake_case(text): # change to regex\n",
    "    if '_' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_filename(text):\n",
    "    if '.c' in text or '.py' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_system_word(text):\n",
    "    system_words = {'git', 'github', 'gitlab', 'ssh', 'intellij', 'server', 'sdl', '@gitlab', 'sdk', 'config'}\n",
    "    \n",
    "    if text in system_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def preprocess_sentence(sentence):\n",
    "    tokenizer = RegexpTokenizer(\"[^;\\s.?,!()]+\\.c|[^;\\s.,?!()]+\\.py|[^;\\s.?!(),]+\\(\\)|[^;\\s.?,!()]+\")\n",
    "#     sentence = [word for word in tokenizer.tokenize(sentence.lower()) if word not in stop_words]\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i].isnumeric():\n",
    "            sentence[i] = \"numericnumber\"\n",
    "        elif is_spelled_out_number(sentence[i]):\n",
    "            sentence[i] = \"nonnumericnumber\"\n",
    "# #         elif is_function(sentence[i]):\n",
    "# #             sentence[i] = \"function\"\n",
    "        elif is_filename(sentence[i]):\n",
    "            sentence[i] = \"filename\"\n",
    "# #         elif is_snake_case(sentence[i]):\n",
    "# #             sentence[i] = \"snakecase\"\n",
    "#         elif is_TA_or_instructor_name(sentence[i]):\n",
    "#             sentence[i] = \"name\"\n",
    "        elif is_system_word(sentence[i]):\n",
    "            sentence[i] = \"sys\"\n",
    "        \n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "    \n",
    "def preprocess_data(x_data, vectorizer=None):\n",
    "    X = None\n",
    "\n",
    "    if vectorizer:\n",
    "        X = vectorizer.transform(x_data)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(preprocessor=preprocess_sentence, ngram_range=(1,2))\n",
    "        X = vectorizer.fit_transform(x_data)\n",
    "        \n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c' 'd' 'sys']\n",
      "[[[37  4]\n",
      "  [ 0 15]]\n",
      "\n",
      " [[20  2]\n",
      "  [ 4 30]]\n",
      "\n",
      " [[49  0]\n",
      "  [ 2  5]]]\n",
      "Training Error: 1.0\n",
      "Test Error: 0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "question_type_data = load_data('../data/questions.json', 'question_type')\n",
    "train_data, test_data = train_test_split(question_type_data)\n",
    "X, vectorizer = preprocess_data(train_data[:,0])\n",
    "y = train_data[:,1]\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "X_test, _ = preprocess_data(test_data[:,0], vectorizer)\n",
    "y_test = test_data[:,1]\n",
    "y_preds = clf.predict(X_test)\n",
    "print(clf.classes_)\n",
    "print(multilabel_confusion_matrix(y_test, y_preds))\n",
    "# print(\"Cross Val Score: {}\".format(cross_val_score(clf, X_test, y_test).mean()))\n",
    "print(\"Training Error: {}\".format(sum(clf.predict(X) == y) / len(y)))\n",
    "print(\"Test Error: {}\".format(sum(y_preds == y_test) / len(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: problems running cache tests with makefile, Prediction: d, Actual: sys\n",
      "Question: I'm getting a heap buffer overflow error when I try to run my lab1 code, which I don't know how to read/deal with , Prediction: c, Actual: d\n",
      "Question: \"bad register name %rflags\", Prediction: c, Actual: d\n",
      "Question: oh no :( we are failing something weird, Prediction: c, Actual: d\n",
      "Question: we are getting a null pointer exception , Prediction: c, Actual: d\n",
      "Question: Tests are passing in IntelliJ but Github says \"commit failed.\" Output also says I won the game even if I haven't., Prediction: d, Actual: sys\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_preds)):\n",
    "    if y_preds[i] != y_test[i]:\n",
    "        print(\"Question: {}, Prediction: {}, Actual: {}\".format(test_data[:,0][i], clf.predict(X_test[i])[0], y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((clf, vectorizer), open('../models/question_type_clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_question_type(question):\n",
    "    clf, vectorizer = pickle.load(open('../models/question_type_clf.pkl', 'rb'))\n",
    "    question, _ = preprocess_data([question], vectorizer)\n",
    "    return clf.predict(question)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_question_type('I have to copy the definition of struct vector into my testing file, in addition to vector.c, to avoid \"incomplete definition\" errors. In the lab, we put it in the header so we didn\\'t have to repeat code. Is that a better way to do this? Or should I put it in both places?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
