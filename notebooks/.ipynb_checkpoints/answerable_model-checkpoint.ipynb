{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semaphore questions : freeing? implementation correctness\n",
      "semaphore questions : freeing ? implementation correctness\n",
      "\n",
      "Being told I need to install developer tools \n",
      " told  need  install developer tools\n",
      "\n",
      "Failing put, remove, containsKey, size in MoveToFrontDictionary\n",
      "failing put , remove , containskey , size  movetofrontdictionary\n",
      "\n",
      "Can't figure out how to pass in arguments to helper function\n",
      "cant figure    pass  arguments  helper function\n",
      "\n",
      "How do I run the testing script on wc.c and can I get general quick feedback on my code for wc.c? Not quite sure what is being looked for during code review.  \n",
      "   run  testing script  wc.c    get general quick feedback   code  wc.c ?  quite sure    looked   code review .\n",
      "\n",
      "test8 still giving memory error\n",
      "test8 still giving memory error\n",
      "\n",
      "b test not passing bc of bug and long loop??\n",
      "\n",
      "b test  passing bc  bug  long loop ? ?\n",
      "\n",
      "I'm testing vector and I'm still not comfortable with c and working from the terminal\n",
      "im testing vector  im still  comfortable  c  working   terminal\n",
      "\n",
      "Want to make sure tier plot is correct. Confused by why it is like this.\n",
      "want  make sure tier plot  correct . confused     like  .\n",
      "\n",
      "We are getting errors with bounce\n",
      "  getting errors  bounce\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(answerable_data[:,0][i])\n",
    "    print(answerable_preprocessor(answerable_data[:,0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answerable_preprocessor(sentence):\n",
    "    sentence = utils.check_quotations(sentence.lower())\n",
    "    sentence = word_tokenize(sentence)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in stop_words:\n",
    "            sentence[i] = \"\"\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_data = utils.load_data('../data/aqfiltered.json', 'answerable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('ngram', CountVectorizer(preprocessor=answerable_preprocessor)),\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('ngram',\n",
       "                                                 CountVectorizer(analyzer='word',\n",
       "                                                                 binary=False,\n",
       "                                                                 decode_error='strict',\n",
       "                                                                 dtype=<class 'numpy.int64'>,\n",
       "                                                                 encoding='utf-8',\n",
       "                                                                 input='content',\n",
       "                                                                 lowercase=True,\n",
       "                                                                 max_df=1.0,\n",
       "                                                                 max_features=None,\n",
       "                                                                 min_df=1,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              1),\n",
       "                                                                 preprocessor=<function answerable_preprocessor at 0x1a18ad0b00>...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight='balanced',\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(answerable_data[:,0], answerable_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6813186813186813"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(pipeline, answerable_data[:,0], answerable_data[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(answerable_data[:,0]).tolist().count('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['?' in answerable_data[i][0] for i in range(len(answerable_data)) if answerable_data[i][1] == 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['?' in answerable_data[i][0] for i in range(len(answerable_data)) if answerable_data[i][1] == 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
